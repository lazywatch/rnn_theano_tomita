import itertools
import random
import argparse
import numpy as np
import six
#from keras.preprocessing.sequence import pad_sequences
# python data_lstar.py --gram 2 --dseed 123
parser = argparse.ArgumentParser(description='Training Sets Generated by Lstar Paper for Tomita grammars')
parser.add_argument('--gram', type=int, default=2, help='index of Tomita grammars')
parser.add_argument('--dseed', type=int, default=123, help='random seed for generating data')

args = parser.parse_args()

if args.dseed < 0:
    args.dseed = 123

random.seed(args.dseed)

def pad_sequences(sequences, maxlen=None, dtype='int32',
                  padding='pre', truncating='pre', value=0.):
    """Pads sequences to the same length.
    This function transforms a list of
    `num_samples` sequences (lists of integers)
    into a 2D Numpy array of shape `(num_samples, num_timesteps)`.
    `num_timesteps` is either the `maxlen` argument if provided,
    or the length of the longest sequence otherwise.
    Sequences that are shorter than `num_timesteps`
    are padded with `value` at the beginning or the end
    if padding='post.
    Sequences longer than `num_timesteps` are truncated
    so that they fit the desired length.
    The position where padding or truncation happens is determined by
    the arguments `padding` and `truncating`, respectively.
    Pre-padding is the default.
    # Arguments
        sequences: List of lists, where each element is a sequence.
        maxlen: Int, maximum length of all sequences.
        dtype: Type of the output sequences.
            To pad sequences with variable length strings, you can use `object`.
        padding: String, 'pre' or 'post':
            pad either before or after each sequence.
        truncating: String, 'pre' or 'post':
            remove values from sequences larger than
            `maxlen`, either at the beginning or at the end of the sequences.
        value: Float or String, padding value.
    # Returns
        x: Numpy array with shape `(len(sequences), maxlen)`
    # Raises
        ValueError: In case of invalid values for `truncating` or `padding`,
            or in case of invalid shape for a `sequences` entry.
    """
    if not hasattr(sequences, '__len__'):
        raise ValueError('`sequences` must be iterable.')
    num_samples = len(sequences)

    lengths = []
    sample_shape = ()
    flag = True

    # take the sample shape from the first non empty sequence
    # checking for consistency in the main loop below.

    for x in sequences:
        try:
            lengths.append(len(x))
            if flag and len(x):
                sample_shape = np.asarray(x).shape[1:]
                flag = False
        except TypeError:
            raise ValueError('`sequences` must be a list of iterables. '
                             'Found non-iterable: ' + str(x))

    if maxlen is None:
        maxlen = np.max(lengths)

    is_dtype_str = np.issubdtype(dtype, np.str_) or np.issubdtype(dtype, np.unicode_)
    if isinstance(value, six.string_types) and dtype != object and not is_dtype_str:
        raise ValueError("`dtype` {} is not compatible with `value`'s type: {}\n"
                         "You should set `dtype=object` for variable length strings."
                         .format(dtype, type(value)))

    x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)
    for idx, s in enumerate(sequences):
        if not len(s):
            continue  # empty list/array was found
        if truncating == 'pre':
            trunc = s[-maxlen:]
        elif truncating == 'post':
            trunc = s[:maxlen]
        else:
            raise ValueError('Truncating type "%s" '
                             'not understood' % truncating)

        # check `trunc` has expected shape
        trunc = np.asarray(trunc, dtype=dtype)
        if trunc.shape[1:] != sample_shape:
            raise ValueError('Shape of sample %s of sequence at position %s '
                             'is different from expected shape %s' %
                             (trunc.shape[1:], idx, sample_shape))

        if padding == 'post':
            x[idx, :len(trunc)] = trunc
        elif padding == 'pre':
            x[idx, -len(trunc):] = trunc
        else:
            raise ValueError('Padding type "%s" not understood' % padding)
    return x

def mean(num_list):
    return sum(num_list)*1.0/len(num_list)

def n_words_of_length(n,length,alphabet):
    if 50*n >= pow(len(alphabet),length):
        res = all_words_of_length(length, alphabet)
        random.shuffle(res)
        return res[:n]
    #else if 50*n < total words to be found, i.e. looking for 1/50th of the words or less
    res = set()
    while len(res)<n:
        word = ""
        for _ in range(length):
            word += random.choice(alphabet)
        res.add(word)
    return list(res)

def all_words_of_length(length,alphabet):
    return [''.join(list(b)) for b in itertools.product(alphabet, repeat=length)]


def compare(network,classifier,length,num_examples=1000,provided_samples=None):
    if not None == provided_samples:
        words = provided_samples
    else:
        words = n_words_of_length(num_examples,length,network.alphabet)
    disagreeing_words = [w for w in words if not (network.classify_word(w) == classifier.classify_word(w))]
    return 1-(len(disagreeing_words)/len(words)), disagreeing_words

def map_nested_dict(d,mapper):
    if not isinstance(d,dict):
        return mapper(d)
    return {k:map_nested_dict(d[k],mapper) for k in d}

class MissingInput(Exception):
    pass

def tomita_1(word):
    return not "0" in word

def tomita_2(word):
    return word=="10"*(int(len(word)/2))

import re
_not_tomita_3 = re.compile("((0|1)*0)*1(11)*(0(0|1)*1)*0(00)*(1(0|1)*)*$")
# *not* tomita 3: words containing an odd series of consecutive ones and then later an odd series of consecutive zeros
# tomita 3: opposite of that
def tomita_3(w):
    return None == _not_tomita_3.match(w) #complement of _not_tomita_3

def tomita_4(word):
    return not "000" in word

def tomita_5(word):
    return (word.count("0")%2 == 0) and (word.count("1")%2 == 0)

def tomita_6(word):
    return ((word.count("0")-word.count("1"))%3) == 0

def tomita_7(word):
    return word.count("10") <= 1

def make_train_set_for_target(target,alphabet,lengths=None,max_train_samples_per_length=300,search_size_per_length=1000,provided_examples=None):
    train_set = {}
    if None == provided_examples:
        provided_examples = []
    if None == lengths:
        lengths = list(range(15)) + [15,20,25,30]
    for l in lengths:
        samples = [w for w in provided_examples if len(w)==l]
        samples += n_words_of_length(search_size_per_length,l,alphabet)
        pos = [w for w in samples if target(w)]
        neg = [w for w in samples if not target(w)]
        pos = pos[:int(max_train_samples_per_length/2)]
        neg = neg[:int(max_train_samples_per_length/2)]
        minority = min(len(pos),len(neg))
        pos = pos[:minority+20]
        neg = neg[:minority+20]
        train_set.update({w:True for w in pos})
        train_set.update({w:False for w in neg})

    print("made train set of size:",len(train_set),", of which positive examples:",
        len([w for w in train_set if train_set[w]==True]))

    return train_set

#curriculum
def mixed_curriculum_train(rnn,train_set,outer_loops=3,stop_threshold=0.001,learning_rate=0.001,
    length_epochs=5,random_batch_epochs=100,single_batch_epochs=100,random_batch_size=20):
    lengths = sorted(list(set([len(w) for w in train_set])))
    for _ in range(outer_loops):
        for l in lengths:
            training = {w:train_set[w] for w in train_set if len(w)==l}
            if len(set([training[w] for w in training])) <= 1: #empty, or length with only one classification
                continue
            rnn.train_group(training,length_epochs,show=False,loss_every=20,stop_threshold=stop_threshold,
                            learning_rate=learning_rate,batch_size=None,print_time=False)
        # all together but in batches
        if rnn.finish_signal == rnn.train_group(train_set,random_batch_epochs,show=True,loss_every=20,
                                                stop_threshold = stop_threshold,
                                                learning_rate=learning_rate,
                                                batch_size=random_batch_size,print_time=False):
            break
        # all together in one batch
        if rnn.finish_signal == rnn.train_group(train_set,single_batch_epochs,show=True,loss_every=20,
                                                stop_threshold = stop_threshold,
                                                learning_rate=learning_rate,batch_size=None,print_time=False): 
            break
    print("classification loss on last batch was:",rnn.all_losses[-1])


def prepare_x_m_y(data_set, curriculum=False):
    x = []
    m = []
    y = []
    if curriculum:
        lengths = sorted(list(set([len(w) for w in data_set])))
        max_len = lengths[-1]
        for l in lengths:
            for w in data_set:
                if len(w) == l:
                    x.append([int(bit) for bit in w])
                    m.append(np.ones_like(x[-1]))
                    y.append([1 if data_set[w] else 0])

            #x_tmp = [[int(bit) for bit in w] for w in data_set if len(w)==l]
            #m_tmp = [np.ones_like(w) for w in x_tmp]
            #y_tmp = [1 if data_set[w] else 0 for w in data_set if len(w) == l]
    else:
        max_len = 0
        for key, value in data_set.iteritems():
            seq_x = [int(bit) for bit in key]
            seq_y = [1] if value else [0]
            x.append(seq_x)
            m.append(np.ones_like(seq_x))
            y.append(seq_y)
            max_len = len(seq_x) if len(seq_x) > max_len else max_len



    x = pad_sequences(x, maxlen=max_len, dtype='int32', padding='post', value=0)
    m = pad_sequences(m, maxlen=max_len, dtype='int32', padding='post', value=0)
    y = np.array(y, dtype='int32').squeeze(axis=1)

    return x, y, m


if args.gram == 1:
    target = tomita_1
    train_lengths = list(range(1,15))# + [16, 19, 22]
    max_train_samples_per_length = 1000
    val_lengths = list(range(1,26,3))
    test_lengths = list(range(1,26))
elif args.gram == 2:
    target = tomita_2
    train_lengths = list(range(1,15))  # + [16, 19, 22]
    max_train_samples_per_length = 1000
    val_lengths = list(range(1,26,3))
    test_lengths = list(range(1,26))
elif args.gram == 3:
    target = tomita_3
    train_lengths = list(range(1,15))  # + [16, 19, 22]
    max_train_samples_per_length = 1000
    val_lengths = list(range(1, 26, 3))
    test_lengths = list(range(1,26))
elif args.gram == 4:
    target = tomita_4
    train_lengths = list(range(1,15))  # + [16, 19, 22]
    max_train_samples_per_length = 1000
    val_lengths = list(range(1, 26, 3))
    test_lengths = list(range(1,26))
elif args.gram == 5:
    target = tomita_5
    train_lengths = list(range(1,15))  # + [16, 19, 22]
    max_train_samples_per_length = 1000
    val_lengths = list(range(1, 26, 3))
    test_lengths = list(range(1,26))
elif args.gram == 6:
    target = tomita_6
    train_lengths = list(range(1,15))  # + [16, 19, 22]
    max_train_samples_per_length = 1000
    val_lengths = list(range(1, 26, 3))
    test_lengths = list(range(1,26))
elif args.gram == 7:
    target = tomita_7
    train_lengths = list(range(1,15))  # + [16, 19, 22]
    max_train_samples_per_length = 1000
    val_lengths = list(range(1, 26, 3))
    test_lengths = list(range(1,26))

max_val_samples_per_length = 500
max_test_samples_per_length = 2000



alphabet = "01"

train_set = make_train_set_for_target(target=target, alphabet=alphabet, lengths=train_lengths, max_train_samples_per_length=max_train_samples_per_length)
val_set = make_train_set_for_target(target=target, alphabet=alphabet, lengths=val_lengths, max_train_samples_per_length=max_val_samples_per_length)
test_set = make_train_set_for_target(target=target, alphabet=alphabet, lengths=test_lengths, max_train_samples_per_length=max_test_samples_per_length)

if args.gram == 5 or args.gram == 6:
    train_x, train_y, train_m = prepare_x_m_y(train_set, True)
else:
    train_x, train_y, train_m = prepare_x_m_y(train_set, False)
val_x, val_y, val_m = prepare_x_m_y(val_set)
test_x, test_y, test_m = prepare_x_m_y(test_set)


train_val_test_file = ''.join(('./data/Tomita/g', str(args.gram), '_train_val_test_data_lstar.npz'))
np.savez(train_val_test_file,
         train_x=train_x, train_m=train_m, train_y=train_y,
         val_x=val_x, val_m=val_m, val_y=val_y,
         test_x=test_x, test_m=test_m, test_y=test_y)
